# Data Engineer Hiring Case Study

## ğŸ“Œ Overview

This project is a **real-time data pipeline** designed to simulate user interactions, stream them through Kafka, process them with real-time aggregations, and visualize the results using a Flask-based dashboard.

## ğŸ“‚ Project Structure

| Directory/File         | Description                                   |
| ---------------------- | --------------------------------------------- |
| **config/**            | Configuration files for MongoDB and Kafka     |
| â”œâ”€â”€ db_config.py      | Configuration for MongoDB                     |
| â”œâ”€â”€ kafka_config.py   | Kafka connection settings                     |
| **dashboard/**         | Contains dashboard-related files              |
| â”œâ”€â”€ templates/        | Stores HTML templates                         |
| â”‚   â”œâ”€â”€ index.html    | Frontend UI for real-time visualization       |
| â”œâ”€â”€ app.py            | Flask application setup                       |
| â”œâ”€â”€ dashboard.py      | Dashboard logic to fetch aggregated data      |
| **.gitignore**         | Specifies files to ignore in version control  |
| **.gitpod.yml**        | Gitpod environment configuration              |
| **consumer.py**        | Kafka consumer for real-time processing       |
| **data_generator.py**  | Generates random interaction data             |
| **producer.py**        | Kafka producer publishing data to Kafka topic |
| **docker-compose.yml** | Docker setup for running services             |
| **requirements.txt**   | Python dependencies                           |
| **setup.sh**           | Setup script to initialize environment        |

---

## ğŸš€ Features

1. **Data Generation & Kafka Streaming**
   - Generates simulated interaction data (user_id, item_id, interaction_type, timestamp).
   - Publishes data to Kafka topic at a controlled rate.

2. **Real-time Data Processing & NoSQL Storage**
   - Kafka consumer fetches and aggregates data (average interactions per user, min/max interactions per item).
   - Stores processed data in **MongoDB**.

3. **Interactive Dashboard for Visualization**
   - Fetches aggregated data from MongoDB.
   - Displays real-time analytics using a Flask-powered web dashboard.

---

## ğŸ“¦ Tech Stack

- **Kafka** - Message streaming
- **MongoDB** - NoSQL storage
- **Flask** - Web framework
- **Docker** - Containerized deployment
- **Gitpod/GitHub** - Development environment

---

## ğŸ› ï¸ Setup & Installation

### Prerequisites

Ensure you have the following installed:

- Docker & Docker Compose
- Python 3.8+
- Kafka & Zookeeper
- MongoDB

### Login to GITPOD

1ï¸âƒ£ Open Gitpod using the following link:
   [Gitpod Workspace](https://gitpod.io/#https://github.com/shivajalabagari/dataengineer_casestudy)

2ï¸âƒ£ Select `PyCharm 2025.1`

3ï¸âƒ£ Choose the standard configuration:
   - **Class:** Up to 4 Cores
   - **Memory:** 8GB RAM
   - **Storage:** 30GB

### Installation Steps

1ï¸âƒ£ Clone the repository:
```bash
git clone <repo-url>
cd <project-folder>
```

2ï¸âƒ£ Install dependencies:
```bash
pip install -r requirements.txt
```

3ï¸âƒ£ Start the services using Docker:
```bash
docker-compose up -d
```

4ï¸âƒ£ Stop the services:
```bash
docker-compose down
```

5ï¸âƒ£ Start the Flask application manually (if needed):
```bash
python dashboard/app.py
```

6ï¸âƒ£ Run the Kafka producer:
```bash
python producer.py
```

7ï¸âƒ£ Start the Kafka consumer:
```bash
python consumer.py
```

8ï¸âƒ£ Launch the Flask dashboard:
```bash
python dashboard/dashboard.py
```

9ï¸âƒ£ Open the browser and visit:
```
http://localhost:5000
```

---

## ğŸ“Š Dashboard Metrics

- **Average Interactions per User** ğŸ“ˆ
- **Max & Min Interactions per Item** ğŸ”¢
- **Live Updates with New Data** ğŸ”„

---
ğŸ¤ Contribution

Feel free to fork this repository, create feature branches, and submit PRs. Suggestions and feedback are always welcome! ğŸ‰


